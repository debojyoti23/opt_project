\documentclass{article} % For LaTeX2e
\usepackage{nips_adapted,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amssymb}
\usepackage{amsmath}

\title{Multivariate Optimization for Non-decomposable Performance Measure}


\author{
Debojyoti Dey \\
15511264 \\
\And
Nimisha Agarwal \\
15511267 \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\DeclareMathOperator*{\argmax}{arg\,max}

\nipsfinalcopy

\begin{document}


\maketitle

\section{Problem Statement}

The goal of the project is to find a general optimization technique to maximize performance measures which are non-decomposable in nature. These are application specific measures and cannot be expressed as a sum of losses on individual data points. One such performance metric is \textit{F-Measure}, expressed as a function of True Positive Rate(TPR) and True Negative Rate(TNR) and often used in class imbalanced classification settings. Another example of non-decomposable performance measure is \textit{precision@k} used in ranking problem where true positiveness of top $k$ elements are to be evaluated. \textit{AUC} is used to compute the area under ROC curve in order to minimize the number of wrongly ordered instances in ranking problem. \textit{pAUC}(partial AUC) is used to limit this evaluation within the region of top-ranked elements. This is the fact that keeps such optimization problems from participating directly in standard batch or online setting of gradient based optimization techniques.

The need for non-additive performance measure derived from the whole sample arises because of the limitation of loss function such as misclassification rate, as it simply predicts the majority class which fails to predict correctly for the instances belonging to minority class in class imbalanced settings. Examples of non-decomposable performance measures include F-measure, G-mean, H-mean etc.

\section{Literature Review}
Several efforts have been made to solve optimization problems with non-decomposable performance measures. They can be broadly categorized into 1) surrogate based approach which tries to minimize hinge loss as an upper bound to binary misclassification in multivariate settings. Cutting plane learning with structural SVM \cite{c4} reduces the problem of minimizing non-decomposable loss to multivariate optimization. 2) Probabilistic plug-in classifier\cite{c3} model tries to find class probability estimator empirically from the training set and maximize the performance measure by obtaining a suitable threshold from test set. 3) Cost-sensitive classification based approach\cite{c6} maximizing performance measure in the form of pseudo linear function on TPR and TNR such as \textit{F-measure}. 4) Stochastic primal dual solver(SPADE)\cite{c2} constructs dual of concave performance measure functions like G-mean, H-mean and performs gradient ascent and gradient descent over primal and dual space respectively. It gives convergence guarantee for loss functions with bounded gradient. Stochastic Alternate Maximization(STAMP)\cite{c2} procedure tries to maximize pseudo linear performance measure in terms of TPR and TNR. \textit{F-measure} is one such candidate eligible for such optimization.

Though there exist stochastic gradient methods such as \textbf{SPADE} which can maximize non-decomposable loss in online fashion\cite{c2} as mentioned above, such methods assume gradient of the loss function to be bounded. Our attempt in this work is to alleviate such restriction and propose a generic model to maximize concave loss function including the ones having large gradient such as that of G-mean.


\section{Multivariate Optmization Setting}
The learning problem tries to find a mapping from input vector $\overline{x} \in \mathcal{X}$ to the label $\overline{y} \in \mathcal{Y}$. We consider our training samples $(x_1,y_1),\ldots,(x_n,y_n) \in \mathcal{X} \times \mathcal{Y}$ drawn $\textit{iid}$ from some unknown underlying distribution $\mathcal{D}$. We are concerned with binary classification with $\mathcal{X} \subseteq \mathbb{R}^d$ and $\mathcal{Y}=\{-1,+1\}$. The goal is to find rule $\overline{h} \in \mathcal{H}$ from hypothesis space $\mathcal{H}$ such that it optimizes the expected prediction performance over $S \in (\mathcal{X} \times \mathcal{Y})^n$. 
\begin{align*}
\overline{h}:\overline{\mathcal{X}} \rightarrow \overline{\mathcal{Y}}
\end{align*}
We define score for a particular input-output combination as follows:
\begin{align*}
f_w(\overline{x},\overline{y}) = w^T\psi({\overline{x},\overline{y}})
\end{align*}
Here $w$ is a parameter vector and $\psi$ is a function that returns a feature vector describing the match between data points $(x_1, \cdots, x_n)$ and $(y_1, \cdots, y_n)$. We have used the following form of $\psi$:
\begin{align*}
\psi(\overline{x},\overline{y}) = \sum_{i=1}^n y_i^*x_i
\end{align*}
Hypothesis function gives $\overline{y}$ with highest score for an input $\overline{x}$ as:
\begin{align*}
\overline{h}_w(\overline{x})=\argmax_{\overline{y}\in\mathcal{Y}}(w^T\psi({\overline{x},\overline{y}}))
\end{align*}

\section{Structural SVM}

Struct SVM is used for multi-class classification. For non-negative $\Delta$, we have used the following formulation of optimization problem \cite{c1}:
\begin{equation*}
\begin{split}
& \underset{w,\xi \geq 0}{min} \quad \frac{1}{2}\|w\|^2+C\xi\\
& s.t \quad \forall\overline{y} \in \overline{\mathcal{Y}}\setminus\overline{y}^*:w^T[\psi(\overline{x},\overline{y}^*)- \psi(\overline{x},\overline{y})]\geq \Delta(\overline{y}^*,\overline{y})-\xi
\end{split}
\end{equation*}
where $\Delta(\overline{y}^*,\overline{y})$ is the loss function. At the solution $w^*, \xi^*$ of the above optimization problem, the value of $\xi^*$ is an upper bound on the loss given by \cite{c1}:
\begin{equation*}
	\Delta(\overline{y}^*,\overline{y}) + \Sigma (y_i - y_i^*)w^Tx_i \leq \xi
\end{equation*}
We want to find such a label for which the loss maximizes. The problem can be formulated as follows:
\begin{equation}
\label{eqn:1}
\mathcal L_w(\overline{x},\overline{y}^*)=\max_{\overline{y}\in\{1,-1\}^n}\{\Delta(\overline{y}^*,\overline{y}) + \sum_{i=1}^n (y_i - y_i^*)w^Tx_i\}
\end{equation}

\section{Formulation of Optimization problem}
We work with concave performance measure which is a function of True Positive rate(TPR) and True Negative Rate(TNR). Performance measure G-mean is given by
\begin{equation*}
\begin{split}
	\phi(P,N) &=\sqrt{PN}\\
	& = \min_{\alpha,\beta}\{\alpha P+\beta N - \phi^*(\alpha,\beta)\}
\end{split}
\end{equation*}
where $\phi$ is a concave function. P and N stands for TPR and TNR respectively. We define our loss function as
\begin{equation*}
\begin{split}
	\Delta(\overline{y}^*,\overline{y}) &= -\phi(P,N) \\
	&= \max_{\alpha,\beta}\{-\alpha P-\beta N + \phi^*(\alpha,\beta)\}\\
\end{split}
\end{equation*}
We can re-write equation \ref{eqn:1} as,
\begin{equation*}
\label{eqn:2}
\begin{split}
	\mathcal L_w(\overline{x},\overline{y}^*)\nonumber &=\max_{\overline{y}\in\{1,-1\}^n}\{\max_{\alpha,\beta}\{-\alpha P-\beta N +\phi^*(\alpha,\beta)\} + \frac{1}{n}\sum_{i=1}^n (y_i - y_i^*)w^Tx_i\}\nonumber\\
	&=\max_{\alpha,\beta}\{\max_{\overline{y}}\{-\alpha P-\beta N  + \frac{1}{n}\sum_{i=1}^n (y_i - y_i^*)w^Tx_i\}+\phi^*(\alpha,\beta)\}
\end{split}
\end{equation*}
We use the notations $y_i^{\star}$ and $y_i$ respectively representing the given and predicted label for $i^{th}$ training instance. $y_i^{\star},y_i\in\mathcal{Y}$, where $\mathcal{Y}=\{-1,+1\}$. TPR and TNR can be represented as the following:
\begin{equation*}
\begin{split}
	&P = \sum_{i=1}^nP_i(y_i,y_i^*)= \frac{1}{n_+}\sum_{i=1}^n\frac{(1+y_i)(1+y_i^*)}{4}\\
	&N = \sum_{i=1}^nN_i(y_i,y_i^*)= \frac{1}{n_-}\sum_{i=1}^n\frac{(1-y_i)(1-y_i^*)}{4}
\end{split}
\end{equation*}
After substituting $P$ and $N$ in equation \ref{eqn:2}, the inner maximization becomes
\begin{equation}
\sum_{i=1}^n\max_{y_i\in\{-1,+1\}}-\frac{\alpha}{n_+}\frac{(1+y_i)(1+y_i^*)}{4} - \frac{\beta}{n_-}\frac{(1-y_i)(1-y_i^*)}{4} + \frac{1}{n}(y_i - y_i^*)w^Tx_i
\end{equation}
Now we need to solve the inner maximization problem. So, we reduce the inner optimization problem to weighted hinge loss which can easily be solved by weighted SVM solver.

Solving the above maximization, we get the following weighted hinge loss like function with some additional constants,
\begin{equation*}
\sum_{i=1}^n
(\frac{\alpha}{n_+}\max\{0,1-y_i^*\frac{2n_+}{\alpha n}w^Tx_i\}
- \frac{\alpha}{n_+})\mathbb{I}(y^*_i=1)\\
+ (\frac{\beta}{n_-}\max\{0,1-y_i^*\frac{2n_-}{\beta n}w^Tx_i\}
- \frac{\beta}{n_-})\mathbb{I}(y^*_i=-1)
\end{equation*}
Following are the steps of the computation:
Now we can re-write equation \ref{eqn:2} as following
\begin{equation*}
\mathcal L_w(\overline{x},\overline{y}^*) = \max_{\alpha,\beta}\{
\frac{\alpha}{n_+}\sum_{y_i^*=1}\max\{0,1-y_i^*\frac{2n_+}{\alpha n}w^Tx_i\}+\frac{\beta}{n_-}\sum_{y_i^*=-1}\max\{0,1-y_i^*\frac{2n_-}{\beta n}w^Tx_i\}-(\alpha+\beta)+\phi^*(\alpha,\beta)\}
\end{equation*}

We can substitute the loss in struct SVM to get the final objective function:
\begin{equation*}
\begin{split}
	&\min_w \frac{||w||^2}{2} + C \mathcal L_w(\overline{x},\overline{y}^*)\\
	&\equiv \min_w \frac{||w||^2}{2} + C\max_{\alpha,\beta}\{
	\frac{\alpha}{n_+}\sum_{y_i^*=1}\max\{0,1-y_i^*\frac{2n_+}{\alpha n}w^Tx_i\}\\
	&+ \frac{\beta}{n_-}\sum_{y_i^*=-1}\max\{0,1-y_i^*\frac{2n_-}{\beta n}w^Tx_i\}\\
	&-(\alpha+\beta)+\phi^*(\alpha,\beta)\}
\end{split}
\end{equation*}

We perform the following steps in each iterative cycle:
\begin{enumerate}
	\item Fix $w$
	\item Gradient ascent on $\mathcal L_w(\overline{x},\overline{y}^*)$ wrt $\alpha,\beta$
	\item Fix $\alpha,\beta$
	\item SVM wrt $w$
	\item Go to step 1
\end{enumerate}
We can use Liblinear solver to perform SVM.

\section{Results}

....


\section{Conclusion}

....

\begin{thebibliography}{99}
	
	\bibitem{c1} Joachims, Thorsten. "A support vector method for multivariate performance measures." Proceedings of the 22nd international conference on Machine learning. ACM, 2005.
	\bibitem{c2} Narasimhan, Harikrishna, Purushottam Kar, and Prateek Jain. "Optimizing Non-decomposable Performance Measures: A Tale of Two Classes." 32nd International Conference on Machine Learning (ICML). 2015.
	\bibitem{c3} Narasimhan, Harikrishna, Rohit Vaish, and Shivani Agarwal. "On the statistical consistency of plug-in classifiers for non-decomposable performance measures." Advances in Neural Information Processing Systems. 2014.
	\bibitem{c4} Joachims, Thorsten, Thomas Finley, and Chun-Nam John Yu. "Cutting-plane training of structural SVMs." Machine Learning 77.1 (2009): 27-59.
	\bibitem{c5} Kar, Purushottam, Harikrishna Narasimhan, and Prateek Jain. "Online and stochastic gradient methods for non-decomposable loss functions." Advances in Neural Information Processing Systems. 2014.
	\bibitem{c6} Parambath, Shameem Puthiya, Nicolas Usunier, and Yves Grandvalet. "Optimizing F-measures by cost-sensitive classification." Advances in Neural Information Processing Systems. 2014.
	
\end{thebibliography}

\end{document}
