\documentclass{article} % For LaTeX2e
\usepackage{nips_adapted,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{algorithm2e}

\title{Multivariate Optimization for Non-decomposable Performance Measure}


\author{
Debojyoti Dey \\
15511264 \\
\And
Nimisha Agarwal \\
15511267 \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\DeclareMathOperator*{\argmax}{arg\,max}

\nipsfinalcopy

\begin{document}


\maketitle

\section{Problem Statement}

The goal of the project is to find a general optimization technique to maximize performance measures which are non-decomposable in nature. These are application specific measures and cannot be expressed as a sum of losses on individual data points. One such performance metric is \textit{F-Measure}, expressed as a function of True Positive Rate(TPR) and True Negative Rate(TNR) and often used in class imbalanced classification settings. Another example of non-decomposable performance measure is \textit{precision@k} used in ranking problem where true positiveness of top $k$ elements are to be evaluated. \textit{AUC} is used to compute the area under ROC curve in order to minimize the number of wrongly ordered instances in ranking problem. \textit{pAUC}(partial AUC) is used to limit this evaluation within the region of top-ranked elements. This is the fact that keeps such optimization problems from participating directly in standard batch or online setting of gradient based optimization techniques.

The need for non-additive performance measure derived from the whole sample arises because of the limitation of loss function such as misclassification rate, as it simply predicts the majority class which fails to predict correctly for the instances belonging to minority class in class imbalanced settings. Examples of non-decomposable performance measures include F-measure, G-mean, H-mean etc.

\section{Literature Review}
Several efforts have been made to solve optimization problems with non-decomposable performance measures. They can be broadly categorized into 1) surrogate based approach which tries to minimize hinge loss as an upper bound to binary misclassification in multivariate settings. Cutting plane learning with structural SVM \cite{c4} reduces the problem of minimizing non-decomposable loss to multivariate optimization. 2) Probabilistic plug-in classifier\cite{c3} model tries to find class probability estimator empirically from the training set and maximize the performance measure by obtaining a suitable threshold from test set. 3) Cost-sensitive classification based approach\cite{c6} maximizing performance measure in the form of pseudo linear function on TPR and TNR such as \textit{F-measure}. 4) Stochastic primal dual solver(SPADE)\cite{c2} constructs dual of concave performance measure functions like G-mean, H-mean and performs gradient ascent and gradient descent over primal and dual space respectively. It gives convergence guarantee for loss functions with bounded gradient. Stochastic Alternate Maximization(STAMP)\cite{c2} procedure tries to maximize pseudo linear performance measure in terms of TPR and TNR. \textit{F-measure} is one such candidate eligible for such optimization.

Though there exist stochastic gradient methods such as \textbf{SPADE} which can maximize non-decomposable loss in online fashion\cite{c2} as mentioned above, such methods assume gradient of the loss function to be bounded. Our attempt in this work is to alleviate such restriction and propose a generic model to maximize concave loss function including the ones having large gradient such as that of G-mean.


\section{Multivariate Optmization Setting}
The learning problem tries to find a mapping from input vector $\overline{x} \in \mathcal{X}$ to the label $\overline{y} \in \mathcal{Y}$. We consider our training samples $(x_1,y_1),\ldots,(x_n,y_n) \in \mathcal{X} \times \mathcal{Y}$ drawn $\textit{iid}$ from some unknown underlying distribution $\mathcal{D}$. We are concerned with binary classification with $\mathcal{X} \subseteq \mathbb{R}^d$ and $\mathcal{Y}=\{-1,+1\}$. The goal is to find rule $\overline{h} \in \mathcal{H}$ from hypothesis space $\mathcal{H}$ such that it optimizes the expected prediction performance over $S \in (\mathcal{X} \times \mathcal{Y})^n$. 
\begin{equation*}
\overline{h}:\overline{\mathcal{X}} \rightarrow \overline{\mathcal{Y}}
\end{equation*}
We define score for a particular input-output combination as follows:
\begin{equation*}
f_w(\overline{x},\overline{y}) = w^T\psi({\overline{x},\overline{y}})
\end{equation*}
Here $w$ is a parameter vector and $\psi$ is a function that returns a feature vector describing the match between data points $(x_1, \cdots, x_n)$ and $(y_1, \cdots, y_n)$. We have used the following form of $\psi$:
\begin{equation*}
\psi(\overline{x},\overline{y}) = \sum_{i=1}^n y_i^*x_i
\end{equation*}
Hypothesis function gives $\overline{y}$ with highest score for an input $\overline{x}$ as:
\begin{equation*}
\overline{h}_w(\overline{x})=\argmax_{\overline{y}\in\mathcal{Y}}(w^T\psi({\overline{x},\overline{y}}))
\end{equation*}

\section{Structural SVM}

Struct SVM by Joachims et al~\cite{c1} is able to address multivariate optimization as modified form of original SVM. For given training labels $\overline{y}^*$ the new optimization problem is defined as following:
\begin{equation*}
\begin{split}
& \underset{w,\xi \geq 0}{min} \quad \frac{1}{2}\|w\|^2+C\xi\\
& s.t \quad \forall\overline{y} \in \overline{\mathcal{Y}}\setminus\overline{y}^*:w^T[\psi(\overline{x},\overline{y}^*)- \psi(\overline{x},\overline{y})]\geq \Delta(\overline{y}^*,\overline{y})-\xi
\end{split}
\end{equation*}
where $\Delta(\overline{y}^*,\overline{y})$ is the loss function. $w^*, \xi^*$ are the solutions of above optimization problem. $\xi^*$ happens to be the upper bound of the struct SVM loss function.
\begin{equation*}
	\Delta(\overline{y}^*,\overline{y}) + \Sigma (y_i - y_i^*)w^Tx_i \leq \xi^*
\end{equation*}
The estimated value of $\xi^*$ is given by the maximum value attained by the LHS of above inequality,
\begin{equation}
\label{eqn:1}
\mathcal L_w(\overline{x},\overline{y}^*)=\max_{\overline{y}\in\{1,-1\}^n}\{\Delta(\overline{y}^*,\overline{y}) + \sum_{i=1}^n (y_i - y_i^*)w^Tx_i\}
\end{equation}

\section{Formulation of Optimization problem}
We work with concave performance measure which is a function of True Positive Rate(TPR) and True Negative Rate(TNR). Performance measure G-mean is given by
\begin{equation*}
\begin{split}
	\phi(P,N) &=\sqrt{PN}\\
	& = \min_{\alpha,\beta}\{\alpha P+\beta N - \phi^*(\alpha,\beta)\}
\end{split}
\end{equation*}
where $\phi^*(\alpha,\beta)$ is the conjugate of concave function $\phi(P,N)$. $\alpha$ and $\beta$ are dual variables. $P$ and $N$ stands for TPR and TNR respectively. We define our loss function as
\begin{equation*}
\begin{split}
	\Delta(\overline{y}^*,\overline{y}) &= -\phi(P,N) \\
	&= \max_{\alpha,\beta}\{-\alpha P-\beta N + \phi^*(\alpha,\beta)\}\\
\end{split}
\end{equation*}
We can re-write equation \ref{eqn:1} as,
\begin{align}
\label{eqn:2}
	\mathcal L_w(\overline{x},\overline{y}^*)\nonumber &=\max_{\overline{y}\in\{1,-1\}^n}\{\max_{\alpha,\beta}\{-\alpha P-\beta N +\phi^*(\alpha,\beta)\} + \frac{1}{n}\sum_{i=1}^n (y_i - y_i^*)w^Tx_i\}\nonumber\\
	&=\max_{\alpha,\beta}\{\max_{\overline{y}}\{-\alpha P-\beta N  + \frac{1}{n}\sum_{i=1}^n (y_i - y_i^*)w^Tx_i\}+\phi^*(\alpha,\beta)\}
\end{align}
TPR and TNR can be represented as the following:
\begin{equation*}
\begin{split}
	&P = \sum_{i=1}^nP_i(y_i,y_i^*)= \frac{1}{n_+}\sum_{i=1}^n\frac{(1+y_i)(1+y_i^*)}{4}\\
	&N = \sum_{i=1}^nN_i(y_i,y_i^*)= \frac{1}{n_-}\sum_{i=1}^n\frac{(1-y_i)(1-y_i^*)}{4}
\end{split}
\end{equation*}
After substituting $P$ and $N$ in equation \ref{eqn:2}, the inner maximization becomes
\begin{equation*}
\sum_{i=1}^n\max_{y_i\in\{-1,+1\}}-\frac{\alpha}{n_+}\frac{(1+y_i)(1+y_i^*)}{4} - \frac{\beta}{n_-}\frac{(1-y_i)(1-y_i^*)}{4} + \frac{1}{n}(y_i - y_i^*)w^Tx_i
\end{equation*}
Solving the above maximization, we get the following weighted hinge loss like function with some additional constants,
\begin{equation*}
\sum_{i=1}^n
(\frac{\alpha}{n_+}\max\{0,1-y_i^*\frac{2n_+}{\alpha n}w^Tx_i\}
- \frac{\alpha}{n_+})\mathbb{I}(y^*_i=1)\\
+ (\frac{\beta}{n_-}\max\{0,1-y_i^*\frac{2n_-}{\beta n}w^Tx_i\}
- \frac{\beta}{n_-})\mathbb{I}(y^*_i=-1)
\end{equation*}

Now we can re-write equation \ref{eqn:2} as following
\begin{align}
\label{eqn:3}
\mathcal L_w(\overline{x},\overline{y}^*) &= \max_{\alpha,\beta}\{
\frac{\alpha}{n_+}\sum_{y_i^*=1}\max\{0,1-y_i^*\frac{2n_+}{\alpha n}w^Tx_i\}\nonumber\\
&+\frac{\beta}{n_-}\sum_{y_i^*=-1}\max\{0,1-y_i^*\frac{2n_-}{\beta n}w^Tx_i\}-(\alpha+\beta)+\phi^*(\alpha,\beta)\}
\end{align}

We can substitute the struct SVM loss in the optimization problem giving the final objective function as:
\begin{equation}
\label{eq:5}
\begin{split}
	\min_w \frac{||w||^2}{2} + C \mathcal L_w(\overline{x},\overline{y}^*)
	&\equiv \min_w \frac{||w||^2}{2} + C\max_{\alpha,\beta}\{
	\frac{\alpha}{n_+}\sum_{y_i^*=1}\max\{0,1-y_i^*\frac{2n_+}{\alpha n}w^Tx_i\}\\
	&+ \frac{\beta}{n_-}\sum_{y_i^*=-1}\max\{0,1-y_i^*\frac{2n_-}{\beta n}w^Tx_i\}-(\alpha+\beta)+\phi^*(\alpha,\beta)\}
\end{split}
\end{equation}
In the next section, we find the conjugate function $\phi^*(\alpha,\beta)$.
\section{Conjugate of G-mean}
For G-mean as $\phi$
\begin{align*}
&\phi(P,N)=\sqrt{PN}\\
&\phi^*(\alpha,\beta) = \min_{P,N}\{\alpha P+\beta N-\sqrt{PN}\}
\end{align*}	
Solving for $g'(P,N)=0$ where $g=\alpha P+\beta N-\sqrt{PN}$, we get $\alpha=\frac{1}{2}\sqrt{\frac{N}{P}}$ and $\beta=\frac{1}{2}\sqrt{\frac{P}{N}}$ giving
\begin{equation}
\phi^*(\alpha,\beta)=0
\end{equation}

\subsection{Dual Feasible Region}
\textbf{Claim 1:} $\alpha > 0,$ $\beta > 0$.\\ \\
\textbf{Proof:} For $\alpha \leq 0$ or $\beta \leq 0$, $P$ and $N$ can be made very large, taking $g$ to -$\infty$. Hence proved.

\textbf{Claim 2:} $\alpha\beta \geq \frac{1}{4}$\\ \\
\textbf{Proof:} We can expand $g$ as follows,
\begin{align*}
g &= \alpha P+\beta N-\sqrt{PN}\\
&=(\sqrt{\alpha P}-\sqrt{\beta N})^2+\sqrt{PN}(2\sqrt{\alpha\beta}-1)
\end{align*}
By ensuring $P=\frac{\beta N}{\alpha}$ we can show the first part to be zero. For large $P,N$, $g \to -\infty$  if $2\sqrt{\alpha\beta}<1$. So $\alpha\beta \geq \frac{1}{4}$. Hence proved.

Using Claim 1 and 2, the feasible dual region can be defined as,
\begin{align*}
dom(\alpha,\beta) = \{\alpha,\beta|\alpha\beta \geq \frac{1}{4},\alpha > 0,\beta > 0\}
\end{align*}

\section{Our Method}
The objective function in~\ref{eq:5} has two components. The inner maximization problem is solved by gradient ascent and the outer minimization is solved by SVM. We initialize $w$ randomly. Then we perform the following steps in each iterative cycle:
\begin{enumerate}
	\item Fix $w$
	\item Gradient ascent on $\mathcal L_w(\overline{x},\overline{y}^*)$ wrt $\alpha,\beta$
	\item Fix $\alpha,\beta$
	\item Perform SVM wrt $w$
	\item Go to step 1
\end{enumerate}
In the second step we perform Gradient ascent with modified gradient step. In the fourth step, we use Liblinear solver to perform SVM. We use weighted hinge loss function.
\subsection{Gradient Ascent Step}
We perform gradient ascent with respect to $\alpha$,$\beta$ for computing $\mathcal L_w(\overline{x},\overline{y}^*)$ defined by equation \ref{eqn:3}. For this purpose we need to find gradient of each constituent term. Say,
\begin{align*}
h(\alpha) = \frac{\alpha}{n_+}\max\{0,1-y_i^*\frac{2n_+}{\alpha n}w^Tx_i\}
\end{align*}

By Danskin's theorem it can be shown that,
\[
h'(\alpha)=
\begin{cases}
0 &\text{if }y_i^*\frac{2n_+}{\alpha n}w^Tx_i \geq 1\\
\frac{1}{n_+} &\text{otherwise}
\end{cases}
\]
Similarly for $h(\beta) = \frac{\beta}{n_-}\max\{0,1-y_i^*\frac{2n_-}{\alpha n}w^Tx_i\}$. 
By Danskin's theorem it can be shown that,
\[
h'(\beta)=
\begin{cases}
0 &\text{if }y_i^*\frac{2n_-}{\beta n}w^Tx_i \geq 1\\
\frac{1}{n_-} &\text{otherwise}
\end{cases}
\]
We have proved $\phi^*(a,b)=0$ for G-mean, hence its derivative gives 0.

Therefore the gradient of $\mathcal L_w(\overline{x},\overline{y}^*)$ wrt dual variables are given by,
\begin{align*}
\bigtriangledown_\alpha =\frac{v_+}{n_+} - 1\\
\bigtriangledown_\beta =\frac{v_-}{n_-} - 1
\end{align*}
where $v_+$ and $v_-$ are the number of positive and negative instances violating the margin.

Hence the update will be as follows:
\begin{align*}
\alpha &= \alpha + \eta \bigtriangledown_\alpha\\
\beta &= \beta + \eta \bigtriangledown_\beta\\
(\alpha,\beta) &= \Pi_{dom(\alpha,\beta)}(\alpha,\beta)
\end{align*}
where $\Pi_\mathcal{C}$ gives the projection onto the convex set $\mathcal{C}$.

We improve the result of gradient ascent by tweaking the computation of gradient. Usually $h'(\alpha)$ (similarly for $h'(\beta)$) for an instance contributes to the final gradient only if hinge loss margin is violated by it. In our method we only add the contribution only if the instance is misclassified i.e. on the wrong side of the hyperplane. We also use dynamic step length using cross-validation to avoid aggressive update.




\begin{figure}
\subfloat[Change of True positive(negative) rate]{
\includegraphics[scale=0.55]{pix/plot_ijcnn.png}
}
\subfloat[Change of G-mean]{
\includegraphics[scale=0.55]{pix/gmean_ijcnn.png}
}
\caption{Accuracy variation for IJCNN data, X-axis represents iteration}
\label{fig:acc_ijcnn}
\end{figure}

\section{Experimental Result}

We have run our experiment on two datasets, a) IJCNN and b) Covertype. We plot change of accuracy with each cycle. We have shown accuracy in terms of TPR and TNR as well as G-mean. Fig \ref{fig:acc_ijcnn} shows the variation of accuracy over the iterations for IJCNN data. Fig \ref{fig:acc_cov} shows accuracy variation for Covertype data. For IJCNN dataset TPR and TNR achieves maximum value of 0.95 and 0.68 respectively.

\begin{figure}
\subfloat[Change of True positive(negative) rate]{
\includegraphics[scale=0.55]{pix/plot_cov.png}
}
\subfloat[Change of G-mean]{
\includegraphics[scale=0.55]{pix/gmean_cov.png}
}
\caption{Accuracy variation for Covertype data, X-axis represents iteration}
\label{fig:acc_cov}
\end{figure}
\section{Conclusion}

....

\begin{thebibliography}{99}
	
	\bibitem{c1} Joachims, Thorsten. "A support vector method for multivariate performance measures." Proceedings of the 22nd international conference on Machine learning. ACM, 2005.
	\bibitem{c2} Narasimhan, Harikrishna, Purushottam Kar, and Prateek Jain. "Optimizing Non-decomposable Performance Measures: A Tale of Two Classes." 32nd International Conference on Machine Learning (ICML). 2015.
	\bibitem{c3} Narasimhan, Harikrishna, Rohit Vaish, and Shivani Agarwal. "On the statistical consistency of plug-in classifiers for non-decomposable performance measures." Advances in Neural Information Processing Systems. 2014.
	\bibitem{c4} Joachims, Thorsten, Thomas Finley, and Chun-Nam John Yu. "Cutting-plane training of structural SVMs." Machine Learning 77.1 (2009): 27-59.
	\bibitem{c5} Kar, Purushottam, Harikrishna Narasimhan, and Prateek Jain. "Online and stochastic gradient methods for non-decomposable loss functions." Advances in Neural Information Processing Systems. 2014.
	\bibitem{c6} Parambath, Shameem Puthiya, Nicolas Usunier, and Yves Grandvalet. "Optimizing F-measures by cost-sensitive classification." Advances in Neural Information Processing Systems. 2014.
	
\end{thebibliography}

\end{document}
